{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [00:12<00:00,  2.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from ast import literal_eval\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM, SFTConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq, DataCollatorForLanguageModeling\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# 1. 설정: pandas 출력 옵션 및 시드 고정\n",
    "pd.set_option('display.max_columns', None)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "def set_seed(random_seed):\n",
    "    torch.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "\n",
    "set_seed(42)\n",
    "# 2. 모델 및 토크나이저 로드 (8-bit 양자화)\n",
    "model_name = \"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # QLoRA는 4bit 양자화를 사용\n",
    "    # load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 계산 precision (float16 또는 bfloat16 사용 가능)\n",
    "    bnb_4bit_use_double_quant=True,       # 이중 양자화 활성화\n",
    "    bnb_4bit_quant_type=\"nf4\"             # NF4 양자화 타입\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,  # BitsAndBytesConfig 추가\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. PEFT 설정 (LoRA)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=['q_proj', 'v_proj', 'k_proj', 'o_proj'],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(dataset, prompts, system_prompt):\n",
    "    records = []\n",
    "    for _, row in dataset.iterrows():\n",
    "        problems = literal_eval(row['problems'])\n",
    "        record = {\n",
    "            'id': row['id'],\n",
    "            'paragraph': row['paragraph'],\n",
    "            'question': problems['question'],\n",
    "            'choices': problems['choices'],\n",
    "            'answer': problems.get('answer', None),\n",
    "            \"question_plus\": problems.get('question_plus', ''),\n",
    "            'klue' : row.get('klue', None),\n",
    "            'question_type' : row.get('question_type', None)\n",
    "        }\n",
    "        records.append(record)\n",
    "            \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    processed_dataset = []\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "        if dataset[i]['question_type'] == '이해형':\n",
    "            prompt = prompts.이해형\n",
    "        elif dataset[i]['question_type'] == '기타':\n",
    "            prompt = prompts.기타\n",
    "        elif dataset[i]['question_type'] == '사실형':\n",
    "            prompt = prompts.사실형\n",
    "        elif dataset[i]['question_type'] == '추론형':\n",
    "            prompt = prompts.추론형\n",
    "        else:\n",
    "            prompt = prompts.나열형\n",
    "        user_message = prompt.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            question_type=dataset[i]['question_type'],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "        # chat message 형식으로 변환\n",
    "        processed_dataset.append(\n",
    "            {\n",
    "                \"id\": dataset[i][\"id\"],\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_message},\n",
    "                    {\"role\": \"assistant\", \"content\": f\"{dataset[i]['answer']}\"}\n",
    "                ],\n",
    "                \"label\": dataset[i]['answer'],\n",
    "                'type' : dataset[i]['question_type']\n",
    "            }\n",
    "        )\n",
    "    processed_dataset = Dataset.from_pandas(pd.DataFrame(processed_dataset))\n",
    "\n",
    "    def formatting_prompts_func(example):\n",
    "        output_texts = []\n",
    "        for i in range(len(example[\"messages\"])):\n",
    "            output_texts.append(\n",
    "                tokenizer.apply_chat_template(\n",
    "                    example[\"messages\"][i],\n",
    "                    tokenize=False,\n",
    "                )\n",
    "            )\n",
    "        return output_texts\n",
    "\n",
    "    def tokenize(element):\n",
    "        outputs = tokenizer(\n",
    "            formatting_prompts_func(element),\n",
    "            truncation=False,\n",
    "            padding=False,\n",
    "            return_overflowing_tokens=False,\n",
    "            return_length=False,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": outputs[\"input_ids\"],\n",
    "            \"attention_mask\": outputs[\"attention_mask\"],\n",
    "        }\n",
    "\n",
    "    # 데이터 토큰화\n",
    "    tokenized_dataset = processed_dataset.map(\n",
    "        tokenize,\n",
    "        remove_columns=list(processed_dataset.features),\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Tokenizing\",\n",
    "    )\n",
    "\n",
    "    # 데이터 분리\n",
    "    tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) <= 2048)  \n",
    "    tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.10, seed=42)\n",
    "\n",
    "    train_dataset = tokenized_dataset['train']\n",
    "    eval_dataset = tokenized_dataset['test']\n",
    "\n",
    "\n",
    "    train_dataset_token_lengths = [len(train_dataset[i][\"input_ids\"]) for i in range(len(train_dataset))]\n",
    "    print(f\"max token length: {max(train_dataset_token_lengths)}\")\n",
    "    print(f\"min token length: {min(train_dataset_token_lengths)}\")\n",
    "    print(f\"avg token length: {np.mean(train_dataset_token_lengths)}\")\n",
    "\n",
    "    # 데이터 확인\n",
    "    return train_dataset, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 2031/2031 [00:02<00:00, 843.20 examples/s] \n",
      "Filter: 100%|██████████| 2031/2031 [00:01<00:00, 1631.54 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max token length: 1561\n",
      "min token length: 247\n",
      "avg token length: 699.9135194307609\n"
     ]
    }
   ],
   "source": [
    "from prompts import user_prompts\n",
    "system_prompt = \"\"\"지시에 따라 주어진 문제의 정답을 고르세요.\"\"\"\n",
    "prompts = user_prompts\n",
    "dataset = pd.read_csv('datas/train+klue.csv')\n",
    "train_dataset, eval_dataset = make_dataset(dataset,prompts, system_prompt)\n",
    "# 여기서 프롬프트만 바꿔서 데이터셋을 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|> system\n",
      "지시에 따라 주어진 문제의 정답을 고르세요.<|im_end|> \n",
      "<|im_start|> user\n",
      "\n",
      "문제 유형:\n",
      "기타\n",
      "\n",
      "지문:\n",
      "본문 1:\n",
      "“전하, 게다가 우리들의 왕국에는 신께 도움이 되지 않는 불편함이 존재합니다. 바로 우리 백성들 중 많은 이들이 그대의 백성들이 가져오는 왕국의 상품과 물건을 간절히 원하고 있다는 것, 그런데 그대의 백성들은 자신들의 탐욕스러운 욕망을 만족시키기 위해 자유민이자 해방된 나의 백성들을 잡아가고 있다는 것입니다. 심지어 귀족과 왕의 친척까지도 잡아가 우리들의 왕국에 있는 백인들에게 팔고 있다는 것입니다.”\n",
      "콩고의 아폰소 1세 국왕이 포르투갈의 주앙 3세 국왕에게 보낸 편지, 1526\n",
      "출처 2:\n",
      "“이번 원정에 많은 비용이 들었기에 빈 손으로 돌아간다면 합리적이지 못한 일이 될 것이다. 우리의 [주된] 바람은 신을 섬기는 것과 콩고 국왕을 기쁘게 하는 것이지만, 그럼에도 불구하고 콩고 국왕으로 하여금 노예가 됐건 구리가 됐건 상아가 됐건 배를 채워야 한다는 사실을 우리들의 이름으로 이해시켜야만 한다.”\n",
      "포르투갈 마누엘 국왕의 콩고에 있는 사절에게 보낸 편지, 1512\n",
      "\n",
      "질문:\n",
      "편지에 설명된 상호 작용은 다음 중 어떤 맥락에서 가장 잘 이해되는가?\n",
      "\n",
      "선택지:\n",
      "1 - 포르투갈의 서아프리카 해안 탐험\n",
      "2 - 사하라 이남 아프리카에서의 가톨릭 선교 활동\n",
      "3 - 사하라 이남 아프리카의 국가 형성\n",
      "4 - 사하라 이남 아프리카의 노예 무역 발전\n",
      "\n",
      "1, 2, 3, 4, 5 중에 하나를 정답으로 고르세요.\n",
      "문제를 풀이할 때, 반드시 지문을 참고하세요.\n",
      "반드시 지문에서 정답의 근거를 찾으세요.\n",
      "\n",
      "출력 형식:\n",
      "근거 : 답변을 도출한 텍스트 # 정답 번호\n",
      "반드시 출력 형식을 지키세요.\n",
      "<|im_end|> \n",
      "<|im_start|> assistant\n",
      "근거 : 내용 형식입니다. 본문에서는 콩고 왕국이 포르투갈에 의해 불법적인 인신매매와 탐욕으로 고통받고 있음을 강조하고, 이에 따른 불만이 표현되고 있습니다. 이를 통해 두 왕국 간의 경제적 착취와 권력의 불균형이 드러나며, 포르투갈이 자원을 착취하기 위한 맥락이 설명됩니다. # 4<|im_end|> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(train_dataset['input_ids'][5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    logits = logits if not isinstance(logits, tuple) else logits[0]\n",
    "    logit_idx = [tokenizer.vocab[\"1\"],\n",
    "                    tokenizer.vocab[\"2\"],\n",
    "                    tokenizer.vocab[\"3\"],\n",
    "                    tokenizer.vocab[\"4\"], \n",
    "                    tokenizer.vocab[\"5\"]]\n",
    "    logits = logits[:,:, logit_idx]\n",
    "    logits = logits[:,-1, :] # -2: answer token, -1: eos token\n",
    "    return logits\n",
    "\n",
    "\n",
    "    # metric 계산 함수\n",
    "def compute_metrics(evaluation_result):\n",
    "    logits, labels = evaluation_result\n",
    "    int_output_map = {\"1\": 0, \"2\": 1, \"3\": 2, \"4\": 3, \"5\": 4}\n",
    "\n",
    "\n",
    "    # 토큰화된 레이블 디코딩\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    labels = [label.split('#')[-1].strip() for label in labels]\n",
    "    labels = [int_output_map.get(label, -1) for label in labels] \n",
    "\n",
    "    # 소프트맥스 함수를 사용하여 로그트 변환\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits, dtype=torch.float32), dim=-1)\n",
    "\n",
    "    predictions = np.argmax(probs, axis=-1)\n",
    "\n",
    "    # 정확도 계산\n",
    "    acc = acc_metric.compute(predictions=predictions, references=labels)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1[\"f1\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "response_template = \"assistant\"\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_seq_length=2048,\n",
    "    output_dir=f\"./outputs + {model_name.split('/')[-1]}\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=1,\n",
    "    max_steps=600,\n",
    "    eval_steps=60,\n",
    "    eval_strategy=\"steps\",  # eval도 step 단위로 수행\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=60,\n",
    "    save_total_limit=3,\n",
    "    save_only_model=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics = preprocess_logits_for_metrics,\n",
    "    args=sft_config,\n",
    "    packing=False,\n",
    "    peft_config = lora_config\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "sft_config.gradient_checkpointing = True\n",
    "sft_config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 2:35:17, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.924800</td>\n",
       "      <td>0.823112</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.141986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.783700</td>\n",
       "      <td>0.788458</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.216025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.876400</td>\n",
       "      <td>0.763525</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.212784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.763200</td>\n",
       "      <td>0.757368</td>\n",
       "      <td>0.524510</td>\n",
       "      <td>0.216474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.803300</td>\n",
       "      <td>0.750436</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>0.252740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.785800</td>\n",
       "      <td>0.746614</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.198501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.743300</td>\n",
       "      <td>0.744389</td>\n",
       "      <td>0.504902</td>\n",
       "      <td>0.221262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.791000</td>\n",
       "      <td>0.743685</td>\n",
       "      <td>0.465686</td>\n",
       "      <td>0.211615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.779300</td>\n",
       "      <td>0.743244</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.204852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.775200</td>\n",
       "      <td>0.743085</td>\n",
       "      <td>0.436275</td>\n",
       "      <td>0.203230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=600, training_loss=0.8215067124366761, metrics={'train_runtime': 9332.6493, 'train_samples_per_second': 0.514, 'train_steps_per_second': 0.064, 'total_flos': 2.1482248155168768e+17, 'train_loss': 0.8215067124366761, 'epoch': 2.6272577996715927})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_config.gradient_checkpointing = True\n",
    "sft_config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from datasets import Dataset\n",
    "\n",
    "def make_test_dataset(test_df, prompt, system_prompt):\n",
    "    # Flatten the JSON dataset\n",
    "    records = []\n",
    "    for _, row in test_df.iterrows():\n",
    "        problems = literal_eval(row['problems'])\n",
    "        record = {\n",
    "            'id': row['id'],\n",
    "            'paragraph': row['paragraph'],\n",
    "            'question': problems['question'],\n",
    "            'choices': problems['choices'],\n",
    "            'answer': problems.get('answer', None),\n",
    "            \"question_plus\": problems.get('question_plus', ''),\n",
    "            'klue': row.get('klue', None),\n",
    "            'question_type': row.get('question_type', None)\n",
    "        }\n",
    "        records.append(record)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    \n",
    "    # test_dataset를 빈 리스트로 초기화\n",
    "    test_dataset = []\n",
    "\n",
    "    # dataset의 각 항목에 대해 처리\n",
    "    for i in range(len(dataset)):\n",
    "        # choices_string 생성\n",
    "        choices_string = \"\\n\".join([f\"{idx + 1} - {choice}\" for idx, choice in enumerate(dataset[i][\"choices\"])])\n",
    "        \n",
    "        # question_type에 맞는 prompt 선택\n",
    "        if dataset[i]['question_type'] == '이해형':\n",
    "            prompt = prompts.이해형\n",
    "        elif dataset[i]['question_type'] == '기타':\n",
    "            prompt = prompts.기타\n",
    "        elif dataset[i]['question_type'] == '사실형':\n",
    "            prompt = prompts.사실형\n",
    "        elif dataset[i]['question_type'] == '추론형':\n",
    "            prompt = prompts.추론형\n",
    "        else:\n",
    "            prompt = prompts.나열형\n",
    "\n",
    "        # user_message 생성\n",
    "        user_message = prompt.format(\n",
    "            paragraph=dataset[i][\"paragraph\"],\n",
    "            question=dataset[i][\"question\"],\n",
    "            question_plus=dataset[i][\"question_plus\"],\n",
    "            question_type=dataset[i]['question_type'],\n",
    "            choices=choices_string,\n",
    "        )\n",
    "\n",
    "        # len_choices는 choices의 길이로 계산\n",
    "        len_choices = len(dataset[i][\"choices\"])\n",
    "\n",
    "        # test_dataset에 항목 추가\n",
    "        test_dataset.append(\n",
    "            {\n",
    "                \"id\": dataset[i][\"id\"],  # dataset[i]에서 'id' 가져오기\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_message},\n",
    "                ],\n",
    "                \"label\": dataset[i].get('answer', None),  # 'answer'가 없으면 None\n",
    "                'type': dataset[i].get('question_type', None),\n",
    "                \"len_choices\": len_choices,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from prompts import user_prompts\n",
    "\n",
    "test_df = pd.read_csv('datas/test_processing_4.csv')\n",
    "prompts = user_prompts\n",
    "test_dataset = make_test_dataset(test_df, prompts, system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'generation-for-nlp-130',\n",
       " 'messages': [{'role': 'system', 'content': '지시에 따라 주어진 문제의 정답을 고르세요.'},\n",
       "  {'role': 'user',\n",
       "   'content': '\\n문제 유형:\\n사실형\\n\\n지문:\\n한  평도  채  안  되는  구 멍가게 는 중풍으로 쓰러져 정상적 건강   상태가 아니었던 아버지의 유일한 수입원이자 생존 이유였다.   때문에 ㉠그 구멍가게에 대한 아버지의 몰두와 자존심은 각별했다 . 한번은 내가 아버지가 가게를 잠깐 비운 사이에 겉에 허연  인공 설탕 가루를 묻힌 ‘미키대장군’ 이라는 캐러멜을 하나 아무  생각 없이 널름 집어먹은 적이 있었다.  하나에 이 원 ,  다섯 개에   십 원이었다.  잠시 뒤에 돌아온 아버지는 단박에 그 사실을 알아 채고는 불같이 화를 내며 내 목덜미에 당수를 한 대 세게 내려 꽂는 것이었다.  그 캐러멜 갑 안에 미키대장군이 몇 개 들어  있는지조차 훤히 꿰차고 있는 아버지였다 . ―이런 민한 종간나래 !  얌생이처럼 기러케 쏠라닥질을 허자면   이 가게 안에 뭐이가 하나 제대로 남아나겠니 ,  응 ? 그러고 나서는 좀 머쓱했는지 입이 한 발쯤 튀어나와 뾰로통 해서 서 있는 내게 미키대장군 네 개를 집어 내미는 거였다 .   어차피 짝이 맞아야 파니까니,  하면서 억지로 내 손아귀에 쥐어 주었다.  ㉡나는 그 무허가 불량 식품인 캐러멜 네 개가 끈끈하게   녹아내릴 때까지 먹지 않고 쥔 채 서 있었다 . ―늴큼 털어 넣지 못하겠니,  으잉? 목덜미에 아버지의 가벼운 당수를 한 대 더 얹은 다음에야  한입에 털어 넣고 돌아서 나왔다.  아버지도 가게 일을 수월하게   보려면 잔심부름꾼인 나를 무시하고는 아쉬울 때가 많을 터였다 .   워낙 짧은 밑천으로 가게를 꾸려 가자니 아버지는 물건 구색을  맞추느라 하루에도 많을 때는 세 번까지 시장통 도매상으로 정부미   포대를 거머쥐고 종종걸음을 쳐야 했고 ,  막내인 나는 번번이  아버지의 뒤로 팔을 늘어뜨린 채  졸졸 따를 수밖에 없었다. 그땐 그게 죽도록 싫었다.  하마 시장통에서 야구 글러브를  끼거나 조립용 신형 무기 장난감 상자를 든 반  친구를 만나거나,   심지어 과외나 주산 학원을 가는 여자 아이들을 만나는 날에는  정말 그 자리에서 혀를 빼물고 죽고 싶은 생각뿐이었다 . (중략)어느 날이었다.  아버지와 나는 앞서거니 뒤서거니 하면서 그  정부미 자루를 날라 왔다.  그런데 집에 도착해 한숨을 돌린 뒤  자루를 풀고 물건을 정리해 보니 스무 병이 와야 할 소주가 두  병이 모자란 채 열여덟 병만 온 것이었다 . ㉢아버지의 얼굴은 맞보기가 민망할 정도로 금세 하얗게 질 렸다.  왜냐하면 그 덜 온 두 병을 빼고 나면 나머지 것들을 몽땅   팔아 봤자 결국 본전치기일 뿐이었기 때문이다.  아버지는 내  등을 떼밀어 물건을 받아 온 수도상회의 혹부리 영감한테 내려  보냈다 .  아버지는 말주변도 말주변이었지만 중풍  후유증  때문에   약간의 언어 장애가 있어 일부러 나를 보냈던 것이다. ―뭐 하러 왔네? 가게 안에 북적거리는 손님들에게 셈을 치러 주느라 몇 번이고   주판알을 고르는 데 바쁜 혹부리 영감의 눈길을 잡아 두는 데  성공한 나는 더듬더듬 자초지종을 말했다 .  그러나 귓등에 연필을   꽂은 채 심술이 덕지덕지 모여 이뤄진 듯한 왼쪽 이마빡의 눈깔 사탕만 한 혹을 어루만지며 듣던 ㉣혹부리 영감은 풍기 때문에   왼쪽으로 힐끗 돌아간 두터운 입술을 떠들쳐 굵은 침방울을 내  얼굴에 마구 튀겼다 .  애초 자기 눈앞에서 까 보이지 않은 것은  인정할 수 없다며 막무가내였다.  나중엔 아버지까지 함께 내려가서   하소연을 해 봤지만 돌아온 대답은 정 그렇게 우기면 거래를  끊겠다는 협박성 경고뿐이었다.  거래가 끊긴다면 아버지한테는  큰 타격이 아닐 수 없었다. 혹부리 영감은 아버지한테 무슨 큰 특혜를 내려 주듯이 거래를   터 준다고 허락을 놓았었다.  같은 함경도 동향이기 때문이라는  말을 덧붙이면서.  하긴 혹부리 영감한테는 매번 소주 열 병 안짝 에다 새우깡 열 봉지 ,  껌 대여섯 개,  빵 예닐곱 개 등 일반 소매 가격 구매자보다 더 많은 물건을 떼어 가지도 않으면서 부득부득   도맷값으로 해  달라고 통사정을 해 쌓는 아버지 같은 사람 하나쯤   거래를 끊어도 장부상 거의 표가 나지 않을 것이었다. 결국 아버지는 자신의 과오를 인정하지 않을 수 없었다.  ㉤당신의   자그마한 구멍가게로 돌아와 나머지 열여덟 병의 소주를 넋 나간   사람처럼 쓰다듬던 아버지는 기어코 아들인 내 앞에서 눈물을  보이고 말았다.  아 !  아버지…….   -김소진 ,  ｢자전거 도둑 ｣ - \\n\\n질문:\\n<보기>를 참고할 때,  ㉠ ～㉤에 대한 반응으로 적절하지 않은   것은?\\n\\n선택지:\\n1 - ㉠ :서술자가 아버지의 내면을 설명하여 독자는 서술자의 해석을   통해 상황을 이해하겠군.\\n2 - ㉡ :서술자가 유년 ‘나 ’의 행위를 묘사하여 독자는 그 행위가  갖는 의미를 스스로 해석하겠군.\\n3 - ㉢ :유년 ‘나 ’로 시선을 제한하여 아버지의 내면이 직접적으로   서술되지 않았다고 생각한 독자라면 아버지의 내면을 스스로  해석하겠군.\\n4 - ㉣ :유년 ‘나 ’로 시선을 제한하여 혹부리 영감의 모습과 행동을   묘사했다고 생각한 독자라면 장면을 직접 보는 듯한 느낌을 받겠군.\\n5 - ㉤ :유년 ‘나 ’로 시선을 제한하여 아버지의 행위와 표정을 묘사 하면서 유년 ‘나 ’의 심리를 함께 제시하여 독자는 그 심리에  공감하겠군.\\n\\n문제 풀이시 다음 지침을 따라주세요:\\n1. 지문을 먼저 읽고 이해하세요.\\n2. 질문을 읽고 이해하세요.\\n3. 지문, 질문을 참고하여 질문에 맞는 답을 찾고, 근거를 제시하세요.\\n4. 선택지를 읽고 질문의 답과 일치하는 선택지를 고르세요.   \\n\\n출력 형식:\\n근거 : 답변을 도출한 텍스트 # 정답 번호\\n반드시 출력 형식을 지키세요.\\n\\n'}],\n",
       " 'label': '',\n",
       " 'type': '사실형',\n",
       " 'len_choices': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "\n",
    "\n",
    "# 배치 데이터 로더를 위한 collate_fn\n",
    "def collate_fn(batch):\n",
    "    ids = [item[\"id\"] for item in batch]\n",
    "    messages = [item[\"messages\"] for item in batch]\n",
    "    labels = [item.get(\"label\", None) for item in batch]  # 라벨이 존재할 경우만 가져옴\n",
    "    return ids, messages, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test dataset inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "  0%|          | 0/435 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 435/435 [10:29:56<00:00, 86.89s/it]   \n"
     ]
    }
   ],
   "source": [
    "from accelerate import Accelerator\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Accelerator 객체 생성\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# GPU 메모리 비우기\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# 데이터 로더 설정\n",
    "batch_size = 2  # 배치 크기 설정\n",
    "dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# 모델과 데이터를 accelerator로 준비\n",
    "model = accelerator.prepare(model)\n",
    "dataloader = accelerator.prepare(dataloader)\n",
    "\n",
    "generated_infer_results = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        ids, messages, labels = batch\n",
    "\n",
    "        # 텍스트 생성을 위한 입력 데이터 준비\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            return_attention_mask=True \n",
    "              # 배치 크기에 맞게 패딩 추가\n",
    "        )\n",
    "\n",
    "        # GPU로 이동 (inputs가 Tensor일 경우 바로 이동)\n",
    "        inputs = inputs.to(model.device)  # Tensor로 직접 처리\n",
    "\n",
    "        # 모델 생성\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=128,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,  # 종료 토큰 설정\n",
    "        )\n",
    "\n",
    "        # 결과 디코딩\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.shape[1]:], skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        for _id, generated_text, label in zip(ids, generated_texts, labels):\n",
    "            generated_infer_results.append({\n",
    "                \"id\": _id,\n",
    "                \"answer\": generated_text,\n",
    "                \"label\": label  # 실제 라벨이 있다면 포함\n",
    "            })\n",
    "\n",
    "# 결과를 DataFrame으로 저장\n",
    "generated_infer_results = pd.DataFrame(generated_infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_infer_results.to_csv('output.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "1    405\n",
       "2    167\n",
       "3    155\n",
       "4    117\n",
       "5     24\n",
       "6      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def extract_last_digit(s):\n",
    "    match = re.search(r'\\d$', s)  # 문자열 끝에서 숫자 하나만 매칭\n",
    "    return match.group() if match else None  # 매칭된 숫자를 반환, 없으면 None 반환\n",
    "\n",
    "result = generated_infer_results\n",
    "# 데이터프레임에 적용\n",
    "result['text'] = result['answer']\n",
    "result['answer'] = result['answer'].apply(lambda x: x.split('#')[-1])\n",
    "result['answer'] = result['answer'].apply(extract_last_digit)\n",
    "result['answer'] = result['answer'].fillna(1)\n",
    "result['answer'] = result['answer'].apply(lambda x: int(x))\n",
    "submission = result[['id', 'text', 'answer']]\n",
    "submission['answer'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "answer\n",
       "1    406\n",
       "2    167\n",
       "3    155\n",
       "4    117\n",
       "5     24\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[['id','answer']].to_csv('output.csv', index = False)\n",
    "submission['answer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Eval dataset으로 inference\n",
    "#### 프롬프트를 바꿔가며 몇개 맞추나 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|██████████| 26/26 [16:46<00:00, 38.71s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.padding_side = 'left'\n",
    "batch_size = 8  # 배치 크기 설정\n",
    "dataloader = DataLoader(eval_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "generated_infer_results = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(dataloader):\n",
    "        ids, messages, labels = batch\n",
    "\n",
    "        # 텍스트 생성을 위한 입력 데이터 준비\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,  # 배치 크기에 맞게 패딩 추가\n",
    "        )\n",
    "\n",
    "        # GPU로 이동 (inputs가 Tensor일 경우 바로 이동)\n",
    "        inputs = inputs.to(model.device)  # Tensor로 직접 처리\n",
    "\n",
    "        # 모델 생성\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_new_tokens=150,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,  # 종료 토큰 설정\n",
    "        )\n",
    "\n",
    "        # 결과 디코딩\n",
    "        generated_texts = tokenizer.batch_decode(\n",
    "            outputs[:, inputs.shape[1]:], skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "\n",
    "        # 결과 저장\n",
    "        for _id, generated_text, label in zip(ids, generated_texts, labels):\n",
    "            generated_infer_results.append({\n",
    "                \"id\": _id,\n",
    "                \"answer\": generated_text,\n",
    "                \"label\": label  # 실제 라벨이 있다면 포함\n",
    "            })\n",
    "\n",
    "# 결과를 DataFrame으로 저장\n",
    "generated_infer_results = pd.DataFrame(generated_infer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import re\n",
    "def extract_last_digit(s):\n",
    "    match = re.search(r'\\d$', s)  # 문자열 끝에서 숫자 하나만 매칭\n",
    "    return match.group() if match else None  # 매칭된 숫자를 반환, 없으면 None 반환\n",
    "\n",
    "result = deepcopy(generated_infer_results)\n",
    "# 데이터프레임에 적용\n",
    "result['text'] = result['answer']\n",
    "result['predict'] = result['answer'].apply(lambda x: x.split('#')[-1])\n",
    "result['predict'] = result['predict'].apply(extract_last_digit)\n",
    "result['predict'] = result['predict'].fillna(1)\n",
    "result['predict'] = result['predict'].apply(lambda x: int(x))\n",
    "\n",
    "result = result[['text', 'predict', 'label',]]\n",
    "result['question_type'] = eval_dataset['type']\n",
    "result['맞았는지 여부'] = result['predict'] == result['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이 문서는 네덜란드의 특권과 헌장에 기초하여, 다양한 신앙을 가진 집단 간의 포용과 공동체의 선을 위한 법적 기준을 설정하는 것을 목표로 두고, 네덜란드 식민지의 다양한 구성원들을 수용하고자 하는 의지를 표현하고 있다.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['text'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "문제 아이디  //   만든 문장   //  만든문장에서뽑은정답  //  정답  //  문제유형  //  맞았는지틀렸는지 \n",
    "\n",
    "\n",
    "df[df['문제유형'] == '추론형']['맞았는지틀렸는지']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
